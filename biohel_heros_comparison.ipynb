{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# BioHEL vs HEROS Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# imports\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "import pickle\n",
                "import time\n",
                "import subprocess\n",
                "import re\n",
                "import json\n",
                "from pathlib import Path\n",
                "from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from openpyxl import Workbook\n",
                "from openpyxl.styles import Font, PatternFill, Alignment\n",
                "from openpyxl.utils.dataframe import dataframe_to_rows\n",
                "\n",
                "# load HEROS\n",
                "try:\n",
                "    from src.skheros.heros import HEROS\n",
                "    print(\"HEROS loaded\")\n",
                "except ImportError:\n",
                "    try:\n",
                "        from skheros.heros import HEROS\n",
                "        print(\"HEROS loaded\")\n",
                "    except ImportError:\n",
                "        print(\"HEROS not found\")\n",
                "\n",
                "BASE_DIR = Path.cwd()\n",
                "DATA_DIR = BASE_DIR / 'biohel_data'\n",
                "DATA_DIR.mkdir(exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# dataset paths\n",
                "DATASET_CONFIGS = {\n",
                "    'MUX6': {\n",
                "        'train_path': 'datasets/multiplexer/A_multiplexer_6_bit_500_inst_CV_Train_1.txt',\n",
                "        'test_path': 'datasets/multiplexer/A_multiplexer_6_bit_500_inst_CV_Test_1.txt',\n",
                "        'excluded_columns': ['Group', 'InstanceID', 'Class'],\n",
                "        'description': '6-bit Multiplexer',\n",
                "        'category': 'Multiplexer',\n",
                "        'features': 6\n",
                "    },\n",
                "    'MUX11': {\n",
                "        'train_path': 'datasets/multiplexer/B_multiplexer_11_bit_5000_inst_CV_Train_1.txt',\n",
                "        'test_path': 'datasets/multiplexer/B_multiplexer_11_bit_5000_inst_CV_Test_1.txt',\n",
                "        'excluded_columns': ['Group', 'InstanceID', 'Class'],\n",
                "        'description': '11-bit Multiplexer',\n",
                "        'category': 'Multiplexer',\n",
                "        'features': 11\n",
                "    },\n",
                "    'MUX20': {\n",
                "        'train_path': 'datasets/multiplexer/C_multiplexer_20_bit_10000_inst_CV_Train_1.txt',\n",
                "        'test_path': 'datasets/multiplexer/C_multiplexer_20_bit_10000_inst_CV_Test_1.txt',\n",
                "        'excluded_columns': ['Group', 'InstanceID', 'Class'],\n",
                "        'description': '20-bit Multiplexer',\n",
                "        'category': 'Multiplexer',\n",
                "        'features': 20\n",
                "    },\n",
                "    'GAM_A': {\n",
                "        'train_path': 'datasets/gametes/A_uni_4add_CV_Train_1.txt',\n",
                "        'test_path': 'datasets/gametes/A_uni_4add_CV_Test_1.txt',\n",
                "        'excluded_columns': ['Class'],\n",
                "        'description': 'GAMETES 4 Additive Univariate',\n",
                "        'category': 'GAMETES',\n",
                "        'features': 100\n",
                "    },\n",
                "    'GAM_C': {\n",
                "        'train_path': 'datasets/gametes/C_2way_epistasis_CV_Train_1.txt',\n",
                "        'test_path': 'datasets/gametes/C_2way_epistasis_CV_Test_1.txt',\n",
                "        'excluded_columns': ['Class'],\n",
                "        'description': 'GAMETES 2-way Epistasis',\n",
                "        'category': 'GAMETES',\n",
                "        'features': 100\n",
                "    },\n",
                "    'GAM_E': {\n",
                "        'train_path': 'datasets/gametes/E_uni_4het_CV_Train_1.txt',\n",
                "        'test_path': 'datasets/gametes/E_uni_4het_CV_Test_1.txt',\n",
                "        'excluded_columns': ['Model', 'InstanceID', 'Class'],\n",
                "        'description': 'GAMETES 4 Heterogeneous Univariate',\n",
                "        'category': 'GAMETES',\n",
                "        'features': 100\n",
                "    }\n",
                "}\n",
                "\n",
                "DATASETS = list(DATASET_CONFIGS.keys())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# helper functions\n",
                "\n",
                "def load_dataset(config):\n",
                "    train_df = pd.read_csv(config['train_path'], sep=\"\\t\")\n",
                "    test_df = pd.read_csv(config['test_path'], sep=\"\\t\")\n",
                "    feature_names = [col for col in train_df.columns if col not in config['excluded_columns']]\n",
                "    X_train = train_df[feature_names].values\n",
                "    y_train = train_df['Class'].values\n",
                "    X_test = test_df[feature_names].values\n",
                "    y_test = test_df['Class'].values\n",
                "    row_id = train_df['InstanceID'].values if 'InstanceID' in train_df.columns else np.arange(len(train_df))\n",
                "    return X_train, y_train, X_test, y_test, row_id, feature_names, train_df, test_df\n",
                "\n",
                "\n",
                "def convert_to_arff(df, feature_names, filename, relation_name):\n",
                "    with open(filename, 'w') as f:\n",
                "        f.write(f\"@RELATION {relation_name}\\n\\n\")\n",
                "        for feat in feature_names:\n",
                "            unique_values = sorted(df[feat].unique())\n",
                "            value_str = ','.join(map(str, [int(v) for v in unique_values]))\n",
                "            f.write(f\"@ATTRIBUTE {feat} {{{value_str}}}\\n\")\n",
                "        class_values = sorted(df['Class'].unique())\n",
                "        class_str = ','.join(map(str, [int(v) for v in class_values]))\n",
                "        f.write(f\"@ATTRIBUTE Class {{{class_str}}}\\n\\n\")\n",
                "        f.write(\"@DATA\\n\")\n",
                "        for _, row in df.iterrows():\n",
                "            values = [str(int(row[feat])) for feat in feature_names]\n",
                "            values.append(str(int(row['Class'])))\n",
                "            f.write(','.join(values) + '\\n')\n",
                "\n",
                "\n",
                "def create_biohel_config(output_path):\n",
                "    config_content = \"\"\"crossover operator 1px\n",
                "default class major\n",
                "fitness function mdl\n",
                "initialization min classifiers 20\n",
                "initialization max classifiers 20\n",
                "iterations 50\n",
                "mdl initial tl ratio 0.25\n",
                "mdl iteration 10\n",
                "mdl weight relax factor 0.90\n",
                "pop size 500\n",
                "prob crossover 0.6\n",
                "prob individual mutation 0.6\n",
                "prob one 0.75\n",
                "selection algorithm tournamentwor\n",
                "tournament size 4\n",
                "windowing ilas 1\n",
                "dump evolution stats\n",
                "smart init\n",
                "class wise init\n",
                "coverage breakpoint 0.01\n",
                "repetitions of rule learning 2\n",
                "coverage ratio 0.90\n",
                "kr hyperrect\n",
                "num expressed attributes init 15\n",
                "hyperrectangle uses list of attributes\n",
                "prob generalize list 0.10\n",
                "prob specialize list 0.10\n",
                "expected number of attributes 10\n",
                "random seed 42\n",
                "\"\"\"\n",
                "    with open(output_path, 'w') as f:\n",
                "        f.write(config_content)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# HEROS training\n",
                "\n",
                "def train_heros(X_train, y_train, row_id, cat_feat_indexes):\n",
                "    heros = HEROS(\n",
                "        outcome_type='class',\n",
                "        iterations=50000,\n",
                "        pop_size=500,\n",
                "        model_iterations=100,\n",
                "        model_pop_size=100,\n",
                "        nu=1,\n",
                "        beta=0.2,\n",
                "        theta_sel=0.5,\n",
                "        cross_prob=0.8,\n",
                "        mut_prob=0.04,\n",
                "        merge_prob=0.1,\n",
                "        subsumption='both',\n",
                "        compaction='sub',\n",
                "        random_state=42,\n",
                "        track_performance=1000,\n",
                "        model_tracking=True,\n",
                "        verbose=True\n",
                "    )\n",
                "    start_time = time.time()\n",
                "    heros.fit(X_train, y_train, row_id, cat_feat_indexes=cat_feat_indexes)\n",
                "    training_time = time.time() - start_time\n",
                "    return heros, training_time\n",
                "\n",
                "\n",
                "def evaluate_heros(heros, X_test, y_test, X_train, y_train):\n",
                "    best_model_idx = heros.auto_select_top_model(X_test, y_test, verbose=False)\n",
                "    predictions = heros.predict(X_test, whole_rule_pop=False, target_model=best_model_idx)\n",
                "    train_predictions = heros.predict(X_train, whole_rule_pop=False, target_model=best_model_idx)\n",
                "    rule_set = heros.get_model_rules(best_model_idx)\n",
                "    return {\n",
                "        'test_accuracy': np.mean(predictions == y_test),\n",
                "        'train_accuracy': np.mean(train_predictions == y_train),\n",
                "        'num_rules': len(rule_set),\n",
                "        'best_model_idx': best_model_idx\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# run HEROS on all datasets\n",
                "heros_results = {}\n",
                "\n",
                "for dataset_name, config in DATASET_CONFIGS.items():\n",
                "    print(f\"\\nProcessing {dataset_name}\")\n",
                "    \n",
                "    dataset_dir = DATA_DIR / dataset_name\n",
                "    dataset_dir.mkdir(exist_ok=True)\n",
                "    \n",
                "    X_train, y_train, X_test, y_test, row_id, feature_names, train_df, test_df = load_dataset(config)\n",
                "    print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
                "    \n",
                "    # create arff files for biohel\n",
                "    convert_to_arff(train_df, feature_names, dataset_dir / 'train.arff', f\"{dataset_name}_Train\")\n",
                "    convert_to_arff(test_df, feature_names, dataset_dir / 'test.arff', f\"{dataset_name}_Test\")\n",
                "    create_biohel_config(dataset_dir / 'config.conf')\n",
                "    \n",
                "    # train\n",
                "    cat_feat_indexes = list(range(X_train.shape[1]))\n",
                "    heros_model, training_time = train_heros(X_train, y_train, row_id, cat_feat_indexes)\n",
                "    \n",
                "    # evaluate\n",
                "    metrics = evaluate_heros(heros_model, X_test, y_test, X_train, y_train)\n",
                "    metrics['training_time'] = training_time\n",
                "    heros_results[dataset_name] = metrics\n",
                "    \n",
                "    # save\n",
                "    with open(dataset_dir / 'heros_model.pickle', 'wb') as f:\n",
                "        pickle.dump(heros_model, f)\n",
                "    \n",
                "    print(f\"Accuracy: {metrics['test_accuracy']:.4f}, Rules: {metrics['num_rules']}, Time: {training_time:.1f}s\")\n",
                "\n",
                "print(\"\\nHEROS done\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# biohel docker wrapper\n",
                "\n",
                "class BioHELDocker:\n",
                "    def __init__(self, base_dir, data_dir):\n",
                "        self.base_dir = Path(base_dir)\n",
                "        self.data_dir = Path(data_dir)\n",
                "        self.docker_image = 'biohel'\n",
                "    \n",
                "    def check_docker(self):\n",
                "        try:\n",
                "            result = subprocess.run(['docker', 'info'], capture_output=True, text=True)\n",
                "            return result.returncode == 0\n",
                "        except FileNotFoundError:\n",
                "            return False\n",
                "    \n",
                "    def build_image(self):\n",
                "        print(\"Building docker image...\")\n",
                "        cmd = ['docker', 'build', '--platform', 'linux/amd64', '-t', self.docker_image, str(self.base_dir)]\n",
                "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
                "        if result.returncode != 0:\n",
                "            print(f\"Build failed: {result.stderr}\")\n",
                "            return False\n",
                "        print(\"Image built\")\n",
                "        return True\n",
                "    \n",
                "    def image_exists(self):\n",
                "        result = subprocess.run(['docker', 'images', '-q', self.docker_image], capture_output=True, text=True)\n",
                "        return bool(result.stdout.strip())\n",
                "    \n",
                "    def run_biohel(self, dataset_name):\n",
                "        dataset_dir = self.data_dir / dataset_name\n",
                "        \n",
                "        for f in ['config.conf', 'train.arff', 'test.arff']:\n",
                "            if not (dataset_dir / f).exists():\n",
                "                print(f\"Missing {f}\")\n",
                "                return None\n",
                "        \n",
                "        print(f\"Running BioHEL on {dataset_name}...\")\n",
                "        start_time = time.time()\n",
                "        \n",
                "        cmd = [\n",
                "            'docker', 'run', '--platform', 'linux/amd64', '--rm',\n",
                "            '-v', f'{dataset_dir.absolute()}:/data',\n",
                "            self.docker_image,\n",
                "            './biohel', '/data/config.conf', '/data/train.arff', '/data/test.arff'\n",
                "        ]\n",
                "        \n",
                "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
                "        elapsed = time.time() - start_time\n",
                "        \n",
                "        if result.returncode != 0:\n",
                "            print(f\"Failed: {result.stderr}\")\n",
                "            return None\n",
                "        \n",
                "        with open(dataset_dir / 'biohel_output.txt', 'w') as f:\n",
                "            f.write(result.stdout)\n",
                "        \n",
                "        return self.parse_output(result.stdout, elapsed)\n",
                "    \n",
                "    def parse_output(self, output, wall_time):\n",
                "        results = {'wall_time': wall_time, 'test_accuracy': None, 'train_accuracy': None, \n",
                "                   'num_rules': 0, 'runtime': None, 'rules': []}\n",
                "        \n",
                "        for line in output.split('\\n'):\n",
                "            if 'Train accuracy :' in line:\n",
                "                match = re.search(r'Train accuracy\\s*:\\s*([\\d.]+)', line)\n",
                "                if match: results['train_accuracy'] = float(match.group(1))\n",
                "            if 'Test accuracy :' in line:\n",
                "                match = re.search(r'Test accuracy\\s*:\\s*([\\d.]+)', line)\n",
                "                if match: results['test_accuracy'] = float(match.group(1))\n",
                "            if 'Total time:' in line:\n",
                "                match = re.search(r'Total time:\\s*([\\d.]+)', line)\n",
                "                if match: results['runtime'] = float(match.group(1))\n",
                "        \n",
                "        # count rules\n",
                "        in_phenotype = False\n",
                "        for line in output.split('\\n'):\n",
                "            if line.startswith('Phenotype:'):\n",
                "                in_phenotype = True\n",
                "                continue\n",
                "            if in_phenotype:\n",
                "                if line.strip() and not line.startswith('Train'):\n",
                "                    results['rules'].append(line.strip())\n",
                "                    results['num_rules'] += 1\n",
                "                else:\n",
                "                    break\n",
                "        \n",
                "        return results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# setup biohel\n",
                "biohel_runner = BioHELDocker(BASE_DIR, DATA_DIR)\n",
                "\n",
                "if not biohel_runner.check_docker():\n",
                "    print(\"Docker not running\")\n",
                "else:\n",
                "    print(\"Docker running\")\n",
                "    if not biohel_runner.image_exists():\n",
                "        biohel_runner.build_image()\n",
                "    else:\n",
                "        print(\"Image exists\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# run biohel on all datasets\n",
                "biohel_results = {}\n",
                "\n",
                "for dataset_name in DATASETS:\n",
                "    print(f\"\\n{dataset_name}\")\n",
                "    result = biohel_runner.run_biohel(dataset_name)\n",
                "    \n",
                "    if result:\n",
                "        biohel_results[dataset_name] = result\n",
                "        print(f\"Acc: {result['test_accuracy']:.4f}, Rules: {result['num_rules']}, Time: {result['runtime']:.1f}s\")\n",
                "    else:\n",
                "        print(\"Failed\")\n",
                "\n",
                "print(\"\\nBioHEL done\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# build comparison dataframe\n",
                "comparison_rows = []\n",
                "\n",
                "for dataset_name, config in DATASET_CONFIGS.items():\n",
                "    if dataset_name in heros_results:\n",
                "        h = heros_results[dataset_name]\n",
                "        comparison_rows.append({\n",
                "            'Dataset': dataset_name,\n",
                "            'Category': config['category'],\n",
                "            'Features': config['features'],\n",
                "            'Algorithm': 'HEROS',\n",
                "            'Test Accuracy': h['test_accuracy'],\n",
                "            'Train Accuracy': h.get('train_accuracy'),\n",
                "            'Num Rules': h['num_rules'],\n",
                "            'Training Time (s)': h['training_time']\n",
                "        })\n",
                "    \n",
                "    if dataset_name in biohel_results:\n",
                "        b = biohel_results[dataset_name]\n",
                "        comparison_rows.append({\n",
                "            'Dataset': dataset_name,\n",
                "            'Category': config['category'],\n",
                "            'Features': config['features'],\n",
                "            'Algorithm': 'BioHEL',\n",
                "            'Test Accuracy': b['test_accuracy'],\n",
                "            'Train Accuracy': b['train_accuracy'],\n",
                "            'Num Rules': b['num_rules'],\n",
                "            'Training Time (s)': b['runtime']\n",
                "        })\n",
                "\n",
                "comparison_df = pd.DataFrame(comparison_rows)\n",
                "comparison_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# summary table\n",
                "print(\"\\nComparison Results\")\n",
                "print(\"-\" * 90)\n",
                "print(f\"{'Dataset':<10} {'Category':<12} {'HEROS Acc':<12} {'BioHEL Acc':<12} {'HEROS Rules':<12} {'BioHEL Rules':<12}\")\n",
                "print(\"-\" * 90)\n",
                "\n",
                "for dataset_name in DATASETS:\n",
                "    config = DATASET_CONFIGS[dataset_name]\n",
                "    h_row = comparison_df[(comparison_df['Dataset'] == dataset_name) & (comparison_df['Algorithm'] == 'HEROS')]\n",
                "    b_row = comparison_df[(comparison_df['Dataset'] == dataset_name) & (comparison_df['Algorithm'] == 'BioHEL')]\n",
                "    \n",
                "    h_acc = f\"{h_row['Test Accuracy'].values[0]:.1%}\" if len(h_row) else \"N/A\"\n",
                "    b_acc = f\"{b_row['Test Accuracy'].values[0]:.1%}\" if len(b_row) else \"N/A\"\n",
                "    h_rules = int(h_row['Num Rules'].values[0]) if len(h_row) else \"N/A\"\n",
                "    b_rules = int(b_row['Num Rules'].values[0]) if len(b_row) else \"N/A\"\n",
                "    \n",
                "    print(f\"{dataset_name:<10} {config['category']:<12} {h_acc:<12} {b_acc:<12} {h_rules:<12} {b_rules:<12}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# plots\n",
                "plot_dir = DATA_DIR / 'comparison_plots'\n",
                "plot_dir.mkdir(exist_ok=True)\n",
                "\n",
                "colors = {'HEROS': '#4472C4', 'BioHEL': '#ED7D31'}\n",
                "\n",
                "# accuracy plot\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "x = np.arange(len(DATASETS))\n",
                "width = 0.35\n",
                "\n",
                "for i, algo in enumerate(['HEROS', 'BioHEL']):\n",
                "    accs = [comparison_df[(comparison_df['Dataset']==d) & (comparison_df['Algorithm']==algo)]['Test Accuracy'].values[0]*100 \n",
                "            if len(comparison_df[(comparison_df['Dataset']==d) & (comparison_df['Algorithm']==algo)]) else 0 \n",
                "            for d in DATASETS]\n",
                "    ax.bar(x + width*(i-0.5), accs, width, label=algo, color=colors[algo])\n",
                "\n",
                "ax.set_xlabel('Dataset')\n",
                "ax.set_ylabel('Test Accuracy (%)')\n",
                "ax.set_title('HEROS vs BioHEL Accuracy')\n",
                "ax.set_xticks(x)\n",
                "ax.set_xticklabels(DATASETS)\n",
                "ax.legend()\n",
                "ax.set_ylim(0, 110)\n",
                "plt.tight_layout()\n",
                "plt.savefig(plot_dir / 'accuracy_comparison.png', dpi=300)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# rules plot\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "\n",
                "for i, algo in enumerate(['HEROS', 'BioHEL']):\n",
                "    rules = [comparison_df[(comparison_df['Dataset']==d) & (comparison_df['Algorithm']==algo)]['Num Rules'].values[0] \n",
                "             if len(comparison_df[(comparison_df['Dataset']==d) & (comparison_df['Algorithm']==algo)]) else 0 \n",
                "             for d in DATASETS]\n",
                "    ax.bar(x + width*(i-0.5), rules, width, label=algo, color=colors[algo])\n",
                "\n",
                "ax.set_xlabel('Dataset')\n",
                "ax.set_ylabel('Number of Rules')\n",
                "ax.set_title('HEROS vs BioHEL Rules')\n",
                "ax.set_xticks(x)\n",
                "ax.set_xticklabels(DATASETS)\n",
                "ax.legend()\n",
                "plt.tight_layout()\n",
                "plt.savefig(plot_dir / 'rules_comparison.png', dpi=300)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save to excel\n",
                "excel_path = DATA_DIR / 'BIOHEL_HEROS_Comparison.xlsx'\n",
                "\n",
                "with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
                "    comparison_df.to_excel(writer, sheet_name='Comparison', index=False)\n",
                "    \n",
                "    pivot = comparison_df.pivot_table(\n",
                "        index=['Dataset', 'Category', 'Features'],\n",
                "        columns='Algorithm',\n",
                "        values=['Test Accuracy', 'Num Rules', 'Training Time (s)'],\n",
                "        aggfunc='first'\n",
                "    )\n",
                "    pivot.to_excel(writer, sheet_name='SideBySide')\n",
                "    \n",
                "    for dataset_name, results in biohel_results.items():\n",
                "        if results.get('rules'):\n",
                "            rules_df = pd.DataFrame({'Rule': results['rules']})\n",
                "            rules_df.to_excel(writer, sheet_name=f'{dataset_name}_BioHEL', index=False)\n",
                "\n",
                "print(f\"Saved: {excel_path}\")\n",
                "\n",
                "# csv\n",
                "csv_path = DATA_DIR / 'comparison.csv'\n",
                "comparison_df.to_csv(csv_path, index=False)\n",
                "print(f\"Saved: {csv_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}